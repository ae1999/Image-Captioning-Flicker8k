{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "undefined-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import tqdm\n",
    "import time\n",
    "import spacy \n",
    "import random\n",
    "import scipy.io\n",
    "import itertools\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from tqdm.contrib import tzip\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "integral-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = \"../../datasets/part1/captions.txt\"\n",
    "images_path = \"../../datasets/part1/Flicker8k_Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smart-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, captions, transform):\n",
    "        self.X, self.y = images, captions\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.y[index]\n",
    "        img = self.X[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, torch.tensor(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "southeast-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils():\n",
    "    def __init__(self, captions_path, images_path, min_word_frequency = 2):\n",
    "        self.min_word_frequency = min_word_frequency\n",
    "        self.max_cap_length = 24\n",
    "        self.english_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        print(\"loading images ...\")\n",
    "        images = self.image_loader(images_path)\n",
    "        print(\"loading captions ...\")\n",
    "        captions = self.caption_loader(captions_path)\n",
    "        \n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        \n",
    "        for i, j in images.items():\n",
    "            self.images.append(j)\n",
    "            self.captions.append(captions[i])\n",
    "        \n",
    "        print(\"creating vocabulary ...\")\n",
    "        self.create_vocabulary(self.captions)\n",
    "        \n",
    "        self.numericalize_captions = self.captions_numericalizer(self.captions)\n",
    "        \n",
    "        print(\"images:\", len(self.images), \"captions:\",  len(self.captions), \"Vocab:\", len(self.vocabulary))\n",
    "    \n",
    "    def captions_numericalizer(self, caps):\n",
    "        numericalize_captions = []\n",
    "        for i in (caps):\n",
    "            items = []\n",
    "            for j in i:\n",
    "                padded_numericalized = self.pad(self.numericalize_caption(j))\n",
    "                items.append(padded_numericalized)\n",
    "            numericalize_captions.append(items)\n",
    "        return numericalize_captions\n",
    "        \n",
    "    def create_vocabulary(self, all_caps):\n",
    "        self.vocabulary = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.rev_vocabulary = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        min_word_frequency = self.min_word_frequency\n",
    "        \n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        counter, count = 1, len(all_caps)\n",
    "        for sentence_list in all_caps:\n",
    "            for sentence in sentence_list:\n",
    "                for word in self.tokenizer_eng(sentence):\n",
    "                    if word not in frequencies:\n",
    "                        frequencies[word] = 1\n",
    "                    else:\n",
    "                        frequencies[word] += 1\n",
    "\n",
    "                    if frequencies[word] == min_word_frequency:\n",
    "                        self.rev_vocabulary[word] = idx\n",
    "                        self.vocabulary[idx] = word\n",
    "                        idx += 1\n",
    "            counter += 1\n",
    "            print(str(round(counter/count*100, 2))+'%', end=\"\\r\")\n",
    "        print('---done!---')\n",
    "        \n",
    "    def pad(self, sequence):\n",
    "        max_cap_length = self.max_cap_length\n",
    "        if len(sequence) > max_cap_length:\n",
    "            sequence = sequence[:max_cap_length]\n",
    "            sequence[max_cap_length-1] = self.rev_vocabulary[\"<EOS>\"]\n",
    "        else:\n",
    "            while(len(sequence) < max_cap_length):\n",
    "                sequence.append(self.rev_vocabulary[\"<PAD>\"])\n",
    "        return sequence\n",
    "        \n",
    "    def tokenizer_eng(self, text):\n",
    "        return [tok.text.lower() for tok in self.english_tokenizer.tokenizer(text)]\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.rev_vocabulary[token] if token in self.rev_vocabulary else self.rev_vocabulary[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def reverse_numericalize(self, encoded):\n",
    "\n",
    "        strings = []\n",
    "            \n",
    "        for token in encoded:\n",
    "            if token in self.vocabulary:\n",
    "                strings.append(self.vocabulary[token])\n",
    "            else:\n",
    "                strings.append(self.vocabulary[3])\n",
    "            if strings[-1] == '<EOS>':\n",
    "                break\n",
    "        \n",
    "        \n",
    "        if '<SOS>' in strings:\n",
    "            strings.remove('<SOS>')\n",
    "        if '<EOS>' in strings:\n",
    "            strings.remove('<EOS>')\n",
    "        while '<PAD>' in strings:\n",
    "            strings.remove('<PAD>')\n",
    "            \n",
    "        sentence = ''\n",
    "        for i in strings:\n",
    "            sentence += i\n",
    "            sentence += ' '\n",
    "            \n",
    "        return sentence[:-1]\n",
    "    \n",
    "    def numericalize_caption(self, caption):\n",
    "        numericalized_caption = [self.rev_vocabulary[\"<SOS>\"]]\n",
    "        numericalized_caption += self.numericalize(caption)\n",
    "        numericalized_caption.append(self.rev_vocabulary[\"<EOS>\"])\n",
    "        return numericalized_caption\n",
    "    \n",
    "    \n",
    "    def get_images_and_captions(self, test_size = 0.2):\n",
    "        sample_captions = []\n",
    "        for i in self.numericalize_captions:\n",
    "            sample_captions.append(i[np.random.randint(5)])\n",
    "        \n",
    "        self.im_train, self.im_test, self.cap_trin, self.cap_test =\\\n",
    "            train_test_split(self.images, sample_captions, test_size = test_size, random_state=42)\n",
    "        return self.im_train, self.im_test, self.cap_trin, self.cap_test\n",
    "    \n",
    "    def image_loader(self, images_path):\n",
    "        images = {}\n",
    "        counter, count = 1, len(os.listdir(images_path))\n",
    "        for image_path in os.listdir(images_path):\n",
    "            images[image_path.split('.')[0]] = imread(images_path + image_path)#cv2.resize(imread(images_path + image_path), (224, 224))\n",
    "            counter+=1\n",
    "            print(round(counter/count*100, 2), '%', end=\"\\r\")\n",
    "        print(\"---done!---\")\n",
    "        return images\n",
    "    \n",
    "    def caption_loader(self, captions_path):\n",
    "        captions = {}\n",
    "        captions_file = pd.read_csv(captions_path).to_numpy()\n",
    "        counter, count = 1, len(captions_file)\n",
    "        for i in captions_file:\n",
    "            \n",
    "            key = i[0].split('.')[0]\n",
    "            \n",
    "            if key in captions.keys():\n",
    "                captions[key].append(i[1])\n",
    "            else:\n",
    "                captions[key] = []\n",
    "                captions[key].append(i[1])\n",
    "            counter+=1\n",
    "            print(str(round(counter/count*100, 2))+'%', end=\"\\r\")\n",
    "        print(\"---done!---\")\n",
    "        return captions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precious-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, t = ''):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg[0], (1, 2, 0)))\n",
    "    plt.title(t)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_x86",
   "language": "python",
   "name": "pytorch_x86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
